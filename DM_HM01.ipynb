{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661cb995",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb234cf3",
   "metadata": {},
   "source": [
    "# <span style=\"color:#3d3d3d\"> Clean the text and create a word cloud </span>\n",
    "# \n",
    "## First install <span style=\"color:red\">wordcloud</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ac1fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb5c119",
   "metadata": {},
   "source": [
    "# <span style=\"color:#252525\"> Let's read <span style=\"color:red\">the 'Data mining' file </span> and clean the lines </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0eda14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file contents\n",
    "\n",
    "with open('Data mining.txt', 'r') as f:\n",
    "        text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d651a24a",
   "metadata": {},
   "source": [
    "# <span style=\"color:#252525\"> I'll be using  <span style=\"color:red\"> The Natural Language Toolkit, or NLTK for short</span>, is a Python library written for working and modeling text.</span>\n",
    "\n",
    "## <span style=\"color:#252525\">It provides good tools for loading and cleaning text that we can use to get our data ready for working with <span style=\"color:red\"> machine learning </span> and <span style=\"color:red\">deep learning </span> algorithms.</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b3a6d4",
   "metadata": {},
   "source": [
    "# <span style=\"color:#252525\"> Import the Necessary libraries </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd87e44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.tokenize import sent_tokenize ,word_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import string\n",
    "\n",
    "#nltk.download('punkt')  # Download the Punkt tokenizer if you haven't done so\n",
    "#nltk.download('stopwords')  # Download the stop words if you haven't done so\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467e2702",
   "metadata": {},
   "source": [
    "## <span style=\"color:#252525\"> sent_tokenize : It's a shortcut for <span style=\"color:red\"> Sentence Tokenization </span> , Which mean <span style=\"color:blue \"> breaks text paragraph into sentences </span>  </span>\n",
    "\n",
    "## <span style=\"color:#252525\"> word_tokenize : It's a shortcut for <span style=\"color:red\"> Word Tokenization </span> , Which mean <span style=\"color:blue \">  breaks text paragraph into words. </span>  </span>\n",
    "\n",
    "## <span style=\"color:#252525\"> stopwords : Stopwords considered as <span style=\"color:red\">noise in the text</span>. Text may contain stop words such as { is, am, are, this, a, an, the, etc }.  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fe5b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the stop words to filter out\n",
    "stop_words=set(stopwords.words('english'))\n",
    "\n",
    "# Split the text into sentences and apply process_txt function on each sentence\n",
    "Sentences = sent_tokenize(text)\n",
    "\n",
    "def process_txt(file_var):\n",
    "        tokens= word_tokenize(file_var.lower())  # tokens is a list of words from the file after converting to lower case\n",
    "        tokens= [word for word in tokens if word.isalpha()]  # to return only the Alphabetical words\n",
    "        tokens= [word for word in  tokens if word not in stop_words] # if the word  considered as a stop_word it won't be returned\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "\n",
    "Words_list = [process_txt(Sentence) for Sentence in Sentences ]\n",
    "\n",
    "Words_list = [x for x in Words_list if x ] # to remove the empty items in the list\n",
    "\n",
    "Words_list = [i for i in Words_list for m in i if len(m) > 3  ] # to remove the items that's shorter than 3 letters from the list\n",
    "# Convert list to a string\n",
    "Words_list = ' '.join([' '.join(string) for string in Words_list])\n",
    "\n",
    "Words_list[:1004]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e3a6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "## Generate a word cloud from the word string\n",
    "wordcloud = WordCloud( background_color='white', min_font_size = 10).generate(Words_list)\n",
    "\n",
    "# Plot the word cloud using matplotlib\n",
    "plt.figure(figsize = (9, 9), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 2) \n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
